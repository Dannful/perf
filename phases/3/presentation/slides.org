# -*- coding: utf-8 -*-
# -*- mode: org -*-
#+startup: beamer overview indent
#+LANGUAGE: pt-br
#+TAGS: noexport(n)
#+EXPORT_EXCLUDE_TAGS: noexport
#+EXPORT_SELECT_TAGS: export

#+Title: *Performance analysis*
#+Subtitle: Step III
#+Author: Francisco Pegoraro Etcheverria, Pedro Henrique Boniatti Colle, Vinícius Daniel Spadotto
#+Date: \today

#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [xcolor=dvipsnames,10pt]
#+OPTIONS: H:1 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+LATEX_HEADER: \input{../../../lib/org-babel.tex}
#+LATEX_HEADER: \usepackage{makecell}

* Introduction

Our object of study is the _[[https://github.com/gabrielfrtg/fletcher-io][Fletcher]]_ application. 
It executes the process of RTM (Reverse Time Migration), which solves 
bidirectional wave equations to create detailed images of the subsoil.

This process consists in solving a determinate number 
of iterations of partial differential equations over a discrete three dimensional grid.


#+latex: \pause

#+attr_latex: :center no :width .51\linewidth
[[../../../img/rtm.jpeg]]

* Related work

** Optimization strategies for geophysics models on manycore systems

Reveals the application's performance on many different architectures, and its limitations.

Fundamentally related to the present work, specially when it comes to the application's I/O
boundness.

* Methodology

The methodology involved executing experiments with pre-defined factors under controlled conditions to observe the application behavior.

** Objectives
   - Configure a controlled environment.
   - Obtain raw data (MSamples/s, execution time and total execution time).
   - Build a linear regression model relating factors to output.


* Methodology
** Experimental Projects
   Two full factorial experimental projects were carried out for both CPU and GPU.

   #+CAPTION: Experimental projects for CPU
   | Backend | Input Data Dimensions                                | Iterations    | Threads        | Replicações |
   |---------+------------------------------------------------------+---------------+----------------+-------------|
   | CPU     | 24^3, 56^3, 120^3                                    | 10, 100, 1000 | 1, 2, 4, 8, 16 |         100 |
   |---------+------------------------------------------------------+---------------+----------------+-------------|
   | GPU     | 24^3, 56^3, 120^3                                    | 10, 100, 1000 | -              |         100 |
   |---------+------------------------------------------------------+---------------+----------------+-------------|
   | CPU     | \makecell{24^3, 88^3, 152^3, \\ 248^3, 376^3, 504^3} | 25            | 1, 2, 4, 8, 16 |          10 |
   |---------+------------------------------------------------------+---------------+----------------+-------------|
   | GPU     | \makecell{24^3, 88^3, 152^3, \\ 248^3, 376^3, 504^3} | 25            | -              |          10 |

* Environment

All experiments were executed at PCAD (https://gppd-hpc.inf.ufrgs.br), utilizing the *draco* and *beagle* machines.

#+CAPTION: Machines used for experiments
| Project | Experiment | Machine |
|---------+------------+---------|
|       1 | CPU        | draco2  |
|       1 | GPU        | draco1  |
|       2 | CPU        | draco2  |
|       2 | GPU        | draco1  |
|       2 | GPU        | beagle  |

* Environment

#+CAPTION: Technical specifications of the machines
| Machine    | CPU                                                                              | RAM                      | Accelerator                                  | Disk                     |
|------------+----------------------------------------------------------------------------------+--------------------------+----------------------------------------------+--------------------------|
| draco[1,2] | \makecell{2 x Intel(R) Xeon(R) \\ E5-2640 v2, 2.00 GHz, \\ 32 threads, 16 cores} | \makecell{64 GB \\ DDR3} | NVIDIA Tesla K20m                            | \makecell{1.8 TB \\ HDD} |
|------------+----------------------------------------------------------------------------------+--------------------------+----------------------------------------------+--------------------------|
| beagle     | \makecell{2 x Intel(R) Xeon(R) \\ E5-2650, 2.00 GHz, \\ 32 threads, 16 cores}    | \makecell{32 GB \\ DDR3} | \makecell{2 x NVIDIA GeForce \\ GTX 1080 Ti} | \makecell{931 GB \\ HDD} |

* Results

#+BEGIN_SRC R :exports none
  source("../analysis/parseExperiments.R")
  source("../analysis/generateAnalysis.R")
#+END_SRC

#+RESULTS:

* Experiment 1: Asymptotic Behavior
  Goal: Validate algorithmic behavior with high replication (100x) on moderate problem sizes ($24^3$ to $120^3$).

* Execution Time (Exp 1)
  Computation (solid) is strictly linear. Total time (dashed) diverges due to disk I/O scaling with problem size/iterations.
  [[file:../analysis/plots/plot1a_execution_vs_total_exp1.pdf]]

* Experiment 2: Large Scale Scalability
  Goal: Stress test with large problem sizes (up to $504^3$) to verify scalability and overhead dilution.

* Execution Time (Exp 2)
  Linear behavior maintained. Total Time slope increases reflecting the massive file writes required for larger grids.
  [[file:../analysis/plots/plot1b_execution_vs_total_exp2.pdf]]

* Overhead (Exp 2)
  Drastic reduction in relative overhead. Useful computation dominates execution in CPU.

  GPU suffers more, due to its proficiency in exploring parallelism.

  [[file:../analysis/plots/plot5b_overhead_percent_exp2.pdf]]

* Speedup (Exp 2)
  Near-linear scaling. Resources effectively converted into performance due to better granularity.
  [[file:../analysis/plots/plot3b_cpu_speedup_exp2.pdf]]

* Efficiency (Exp 2)
  High efficiency (>80%) retained even at 16 threads. Application is weakly scalable.
  [[file:../analysis/plots/plot4b_cpu_efficiency_exp2.pdf]]

* Throughput (Exp 2)
GPU dominates the general throughput, displaying a resource saturation at problem size $\approx 150$.
  [[file:../analysis/plots/plot2b_throughput_exp2.pdf]]

* Regression Model (CPU)
  High accuracy ($R^2 \approx 0.99$) achieved for CPU execution time modeling.
  #+attr_latex: :height 0.8\textheight
  [[file:../analysis/plots/lm_cpu_diagnostics.pdf]]

* Regression Model (Draco GPU)
  Predictive model for the Draco machine (Tesla K20m).
  [[file:../analysis/plots/lm_draco_gpu_diagnostics.pdf]]

* Regression Model (Beagle GPU)
  Predictive model for the Beagle machine (GTX 1080 Ti).
  [[file:../analysis/plots/lm_beagle_gpu_diagnostics.pdf]]

* Conclusion

- *Robustness Verified:* Confirmed $O(n)$ complexity and predictable behavior across all scales.
- *Overhead Dynamics:* High relative overhead on small inputs limits speedup, but is effectively diluted in larger workloads. This phenomenon in GPU, however, requires substantially larger problem sizes, bound by its VRAM. Furthermore, the disk I/O becomes a massive bottleneck as parallelism increases.
- *High Scalability:* Achieved near-linear speedup and >80% efficiency on large problems, validating HPC readiness.
- *Future Work:* Solid foundation established for distributed memory implementation (MPI).

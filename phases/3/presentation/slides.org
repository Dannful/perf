# -*- coding: utf-8 -*-
# -*- mode: org -*-
#+startup: beamer overview indent
#+LANGUAGE: pt-br
#+TAGS: noexport(n)
#+EXPORT_EXCLUDE_TAGS: noexport
#+EXPORT_SELECT_TAGS: export

#+Title: *Performance analysis*
#+Subtitle: Step II
#+Author: Francisco Pegoraro Etcheverria, Pedro Henrique Boniatti Colle, Vin√≠cius Daniel Spadotto
#+Date: \today

#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [xcolor=dvipsnames,10pt]
#+OPTIONS: H:1 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+LATEX_HEADER: \input{../../../lib/org-babel.tex}
#+LATEX_HEADER: \usepackage{makecell}

* Introduction

Our object of study is the _[[https://github.com/gabrielfrtg/fletcher-io][Fletcher]]_ application. 
It executes the process of RTM (Reverse Time Migration), which solves 
bidirectional wave equations to create detailed images of the subsoil.

This process consists in solving a determinate number 
of iterations of partial differential equations over a discrete three dimensional grid.


#+latex: \pause

#+attr_latex: :center no :width .51\linewidth
[[../../../img/rtm.jpeg]]

* Methodology

The methodology involved executing experiments with pre-defined factors under controlled conditions to observe the application behavior.

** Objectives
   - Configure a controlled environment.
   - Obtain raw data (MSamples/s, execution time and total execution time).
   - Build a linear regression model relating factors to output.

** Experimental Projects
   Two full factorial experimental projects were carried out for both CPU and GPU.

   #+CAPTION: Experimental projects for CPU
   | Project | Input Data Dimensions                  | Iterations    | Threads        |
   |---------+----------------------------------------+---------------+----------------+
   |       1 | 24^3, 56^3, 120^3                      | 10, 100, 1000 | 1, 2, 4, 8, 16 |
   |       2 | 24^3, 88^3, 152^3, 248^3, 376^3, 504^3 | 25            | 1, 2, 4, 8, 16 |

   with $100$ and $10$ replications, respectively.

* Methodology

** Experimental Projects

   #+CAPTION: Experimental projects for GPU
   | Project | Input Data Dimensions                  | Iterations    | Replications |
   |---------+----------------------------------------+---------------+--------------|
   |       1 | 24^3, 56^3, 120^3                      | 10, 100, 1000 | 100          |
   |       2 | 24^3, 88^3, 152^3, 248^3, 376^3, 504^3 | 25            | 10           |

* Environment

All experiments were executed at PCAD (https://gppd-hpc.inf.ufrgs.br), utilizing the *draco* and *beagle* machines.

#+CAPTION: Machines used for experiments
| Project | Experiment | Machine |
|---------+------------+---------|
|       1 | CPU        | draco2  |
|       1 | GPU        | draco1  |
|       2 | CPU        | draco2  |
|       2 | GPU        | draco1  |
|       2 | GPU        | beagle  |

* Environment

#+CAPTION: Technical specifications of the machines
| Machine    | CPU                                                                              | RAM                      | Accelerator                                  | Disk                     |
|------------+----------------------------------------------------------------------------------+--------------------------+----------------------------------------------+--------------------------|
| draco[1,2] | \makecell{2 x Intel(R) Xeon(R) \\ E5-2640 v2, 2.00 GHz, \\ 32 threads, 16 cores} | \makecell{64 GB \\ DDR3} | NVIDIA Tesla K20m                            | \makecell{1.8 TB \\ HDD} |
|------------+----------------------------------------------------------------------------------+--------------------------+----------------------------------------------+--------------------------|
| beagle     | \makecell{2 x Intel(R) Xeon(R) \\ E5-2650, 2.00 GHz, \\ 32 threads, 16 cores}    | \makecell{32 GB \\ DDR3} | \makecell{2 x NVIDIA GeForce \\ GTX 1080 Ti} | \makecell{931 GB \\ HDD} |

* Results

#+BEGIN_SRC R :exports none
  source("../analysis/parseExperiments.R")
  source("../analysis/generateAnalysis.R")
#+END_SRC

* Experiment 1: Asymptotic Behavior
  Goal: Validate algorithmic behavior with high replication (100x) on moderate problem sizes ($24^3$ to $120^3$).

* Execution Time (Exp 1)
  Rigorous linearity confirms $O(n)$ complexity. Error bars are negligible due to high replicability.
  [[file:../analysis/plots/plot1a_execution_vs_total_exp1.pdf]]

* Overhead (Exp 1)
  Significant relative overhead for small inputs. Computation time is insufficient to mask management costs.
  [[file:../analysis/plots/plot5a_overhead_percent_exp1.pdf]]

* Speedup (Exp 1)
  Saturates early for small problems ($24^3$). Communication costs negate benefits of additional threads.
  [[file:../analysis/plots/plot3a_cpu_speedup_exp1.pdf]]

* Efficiency (Exp 1)
  Sharp drops with increased threads highlight underutilization in fine-grained tasks.
  [[file:../analysis/plots/plot4a_cpu_efficiency_exp1.pdf]]

* Throughput (Exp 1)
  Saturates early, limited by system overheads relative to the small workload.
  [[file:../analysis/plots/plot2a_throughput_exp1.pdf]]

* Experiment 2: Large Scale Scalability
  Goal: Stress test with large problem sizes (up to $504^3$) to verify scalability and overhead dilution.

* Execution Time (Exp 2)
  Linear behavior $O(n)$ maintained under intense load. No degradation observed.
  [[file:../analysis/plots/plot1b_execution_vs_total_exp2.pdf]]

* Overhead (Exp 2)
  Drastic reduction in relative overhead. Useful computation dominates execution.
  [[file:../analysis/plots/plot5b_overhead_percent_exp2.pdf]]

* Speedup (Exp 2)
  Near-linear scaling. Resources effectively converted into performance due to better granularity.
  [[file:../analysis/plots/plot3b_cpu_speedup_exp2.pdf]]

* Efficiency (Exp 2)
  High efficiency (>80%) retained even at 16 threads. Application is weakly scalable.
  [[file:../analysis/plots/plot4b_cpu_efficiency_exp2.pdf]]

* Throughput (Exp 2)
  Maximum performance achieved by saturating FP units and masking memory latency.
  [[file:../analysis/plots/plot2b_throughput_exp2.pdf]]

* Conclusion

- *Robustness Verified:* Confirmed $O(n)$ complexity and predictable behavior across all scales.
- *Overhead Dynamics:* High relative overhead on small inputs limits speedup, but is effectively diluted in larger workloads.
- *High Scalability:* Achieved near-linear speedup and >80% efficiency on large problems, validating HPC readiness.
- *Future Work:* Solid foundation established for distributed memory implementation (MPI).
